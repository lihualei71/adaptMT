---
title: "Introduction to `adaptMT` package"
author: "Lihua Lei"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction
This is a demo of our `adaptMT` package. `adaptMT` has two main components: a generic interface that allows users to specify the working model and algorithms to fit them, as well as a pool of easy-to-use end-to-end wrappers. The former is captured by function `adapt`. The latter includes `adapt_glm`, `adapt_gam` and `adapt_glmnet` for using GLM, GAM and L1-regularized GLM. 

In this section, we illustrate `adaptMT` package on an example.

```{r, results='hide'}
# install the "adaptMT" package from github.
# will be submitted to CRAN very soon.
# devtools::install_github("lihualei71/adaptMT")
library("adaptMT")
```

We illustrate one of the main function `adapt_glm`, for AdaPT with logistic-Gamma GLM as the working model, on `estrogen` dataset, a gene/drug response dataset from NCBI Gene Expression Omnibus (GEO). `estrogen` dataset consists of gene expression measurements for $n = 22283$ genes, in response to estrogen treatments in breast cancer cells for five groups of patients, with different dosage levels and 5 trials in each. The task is to identify the genes responding to a low dosage. The p-values pi for gene i is obtained by a one-sided permutation test which evaluates evidence for a change in gene expression level between the control group (placebo) and the low-dose group. $\{p_i : i \in [n]\}$ are then ordered according to permutation t-statistics comparing the control and low-dose data, pooled, against data from a higher dosage (with genes that appear to have a strong response at higher dosages placed earlier in the list). The code to compute p-values and the ordering can be found in [Rina Barber's website](http://www.stat.uchicago.edu/~rina/sabha/gene_drug_data_example.R).

In this demo, we subsample the top 5000 genes for illustration.

```{r,results='hide'}
# load the data.
data("estrogen")
# Take the first 5000 genes
if (!requireNamespace("dplyr")){
  install.packages("dplyr")
}
library("dplyr")
estrogen <- select(estrogen, pvals, ord_high) %>% 
  filter(ord_high <= 5000)
rownames(estrogen) <- NULL
```

```{r}
head(estrogen, 5)
```

```{r}
summary(estrogen)
```

```{r, echo=FALSE, fig.height=5, fig.width=7}
  plot(estrogen$ord, estrogen$pvals, pch = ".", xlab = "order", ylab = "p-values", xaxs = "i", yaxs = "i", main = "Scatter plot of p-values in the (sub-sampled) estrogen dataset")
```

Now we execute `adapt_glm` on this dataset. `adapt_glm` assumes a conditional logistic-Gamma GLM as the working model. Specifically, it models the p-values as
$$H_i \mid x_i \sim \pi(x_i), \quad \mathrm{logit}(\pi(x_i))= \phi(x_i)^{T}\beta$$
$$-\log p_i \mid H_i, x_i\sim \left\{\begin{array}{ll} \mathrm{Exp(1)} & H_i = 0\\ \mathrm{Exp(\mu(x))} & H_i = 1\end{array}\right., \quad \frac{1}{\mu(x_i)} = \phi(x_i)^{T}\gamma$$
where $\phi(x)$ is a featurization of $x$. In this example, we use the spline bases, given by `ns` function from `splines` package. For illustration, we choose our candidate models as the above GLMs with $\phi(x)$ being the spline bases with equal-spaced knots and the number of knots ranging from 6-10. We use BIC to select the best model at the initial stage and use the selected model for the following model fitting.

```{r}
# prepare the inputs of AdaPT
# need "splines" package to construct the formula for glm
library("splines")
pvals <- as.numeric(estrogen$pvals)
x <- data.frame(x = as.numeric(estrogen$ord))
formulas <- paste0("ns(x, df = ", 6:10, ")")
formulas
```

`adapt_glm` function provides several user-friendly tools to monitor the progress. For model selection, a progress bar will, by default, be shown in the console that indicates how much proportion of models have been fitted. This can be turned off by setting verbose\$ms = FALSE. Similarly for model fitting, a progress bar can be shown in the console, though not by default, by setting verbose\$fit = TRUE. Also, by default, the progress of the main process will be shown in the console that indicates (1) which target FDR level has been achieved; (2) FDPhat for each target FDR level; (3) number of rejections for each target FDR level.

```{r, warning=FALSE}
# run AdaPT
res <- adapt_glm(x = x, pvals = pvals, pi_formulas = formulas, mu_formulas = formulas)
```

`plot_thresh_1d` gives the plot for the rejection threshold as a function of x (must be univariate without repeated value) for given $\alpha$. We display the plots for $\alpha \in \{0.3, 0.25, 0.2, 0.15, 0.1, 0.05\}$.

```{r, echo=FALSE, fig.height=5, fig.width=7}
par(mfrow = c(2, 3))
for (alpha in seq(0.3, 0.05, -0.05)){
  nrejs <- res$nrejs[floor(alpha * 100)]
  title <- paste0("alpha = ", alpha, ", nrejs = ", nrejs)
  plot_1d_thresh(res, alpha, title, disp_ymax = 1, xlab = "order")
}
```

`plot_lfdr_1d` gives the plot for the estimated local FDR as a function of x (must be univariate without repeated value) for given $\alpha$. We display the plots for $\alpha \in \{0.3, 0.25, 0.2, 0.15, 0.1, 0.05\}$. It is clear that the estimated local FDR almost remains the same, indicating that the information loss caused by partial masking is small.

```{r, fig.height=5, fig.width=7}
par(mfrow = c(2, 3))
for (alpha in seq(0.3, 0.05, -0.05)){
  nrejs <- res$nrejs[floor(alpha * 100)]
  title <- paste0("alpha = ", alpha, ", nrejs = ", nrejs)
  plot_1d_lfdr(res, alpha, title, disp_ymax = 0.25, xlab = "order")
}
```

# Generic Interface of `adaptMT` Package
The pipeline of AdaPT is as follows:

```{r, out.width = "100%", fig.align = "center", echo = FALSE}
knitr::include_graphics('flowchart_AdaPT.pdf')
```


The data-adaptive/human-in-the-loop part is embedded in the EM sub-pipeline:

```{r, out.width = "100%", fig.align = "center", echo = FALSE}
knitr::include_graphics('flowchart_EM.pdf')
```

```{r, out.width = "100%", fig.align = "center", echo = FALSE}
knitr::include_graphics('flowchart_EM_ms.pdf')
```

There are two main user-specified options:
- exponential family to fit E-steps;
- functions to fit $\pi_{1}(x)$ and $\mu(x)$, including those for initialization.

We introduce each of them in the following subsections. In the final subsection we briefly introduce other arguments (of `adapt`)

## `exp_family` objects for fitting E-steps
An `exp_family` object is a list containing all required information for an exponential family, with mean-parameter $\mu$ and natural parameter $\eta = \eta(\mu)$
$$h(p; \mu) = \exp\left\{(\eta(\mu) - \eta(\mu^{*}))g(p) - (A(\mu) - A(\mu^{*}))\right\}.$$
The above formulation exploits an over-parametrization by introducing $\mu^{*}$ to guarantee that $U([0, 1])$ belongs to the family and $h(p; \mu^{*})$ gives its density. Here by mean-parameter we mean by convention that
$$E_{\mu}[g(p)] = \mu.$$

`adaptMT` package provides a generic function `exp_family` to construct an `exp_family` object from scratch. The required arguments include: (1) `g` as a function; (2) `ginv` as the inverse function of `g`; (3) `eta` as a function; (4) `mustar` as a numeric; (5) `A` as a function. 

The current version of `adaptMT` package provides two instances: `beta_family()` and `inv_gaussian_family()`. For `beta_family()`, 
$$g(p) = -\log(p), \,\,\,\, g^{-1}(x) = e^{-x}, \,\,\,\, \eta(\mu) = -\frac{1}{\mu}, \,\,\,\, \mu^{*} = 1, \,\,\,\, A(\mu) = \log(\mu).$$
For `inv_gaussian_family()`, 
$$g(p) = \Phi^{-1}(1 - p), \,\,\,\, g^{-1}(x) = 1 - \Phi(x), \,\,\,\, \eta(\mu) = \mu, \,\,\,\, \mu^{*} = 0, \,\,\,\, A(\mu) = \frac{1}{2}\mu^{2}.$$
We found that `beta_family()` consistently yields better results. Even in the case that the p-values are generated from a normal model as `inv_gaussian_family()` specifies, `beta_family()` still has comparable performance. For this reason, we take `dist = beta_family()` as the default input in `adapt` and all its wrappers.

`adaptMT` package provides a generic function to generate an 'exp_family' object based on these inputs. For instance, the following code generates `beta_family()`, 
```{r}
# g function
g <- function(x){
    tmp <- -log(x)
    pmax(pmin(tmp, -log(10^-15)), -log(1-10^-15))
}
# inverse function of g
ginv <- function(x){
    tmp <- exp(-x)
    pmax(pmin(tmp, 1 - 10^-15), 10^-15)
}
# eta function (mapping from the mean parameter to the natural parameter)
eta <- function(mu){-1/mu}
# mu* (the mean parameter corresponding to U([0, 1]))
mustar <- 1
# A function (the partition function)
A <- log
# Set a name; optional
name <- "beta"
# Set the default family as Gamma() (for glm, gam, glmnet, etc.); optional
family <- Gamma()
# Generate beta_family()
beta_family <- gen_exp_family(g, ginv, eta, mustar, A, name, family)
```


## `adapt_model` objects for fitting M-steps
An `adapt_model` object is a list containing the fitting functions (and their arguments) in M-steps. It has three entries: `name`, `algo` and `args`. `name` gives the name of the model, which is optional; `algo` is a list of four functions `pifun`, `mufun`, `pifun_init` and `mufun_init`, for fitting $\pi_{1}(x)$ and $\mu(x)$ and initializing $\pi_{1}(x)$ and $\mu(x)$, respectively; `args` is a list of four lists  `piargs`, `muargs`, `piargs_init` and `muargs_init` that give the other arguments passed to `pifun`, `mufun`, `pifun_init` and `mufun_init`. `adaptMT` package provides a generic function that takes the aforementioned elements as input to generate an `adapt_model` object, that can be passed into `adapt` or its variants. 

### Design principles and requirements for `pifun` and `mufun`
Let $P^{(r-1)}[\cdot \mid D_{t}]$ denotes the posterior probability at step $t$ after $(r-1)$ iteration in the EM algorithm. Here $D_{t}$ denotes the partially-masked dataset, namely $(x_i, \tilde{p}_{t, i})_{i\in [n]}$. Let
$$\hat{H}_{i}^{r} = P^{(r-1)} [H_{i} = 1\mid D_{t}], \quad \hat{b}_{i}^{r} = P^{(r-1)}[p_{i}\le 0.5 \mid D_{t}]$$
To fit $\pi_{1}(x)$ in the $r$-th iteration, one can construct a pseudo-dataset with
$$y_{\pi}^{r} = (1, \ldots, 1, 0, \ldots, 0)\in R^{2n},$$
with weights 
$$w_{\pi}^{r} = \left(\hat{H}_{1}^{r}, \ldots, \hat{H}_{n}^{r}, 1 - \hat{H}_{1}^{t}, \ldots, 1 - \hat{H}_{n}^{r}\right).$$
Let $xx$ be the covariate matrix after replicated for twice, namely `rbind(x, x)`. Then one can apply any classification algorithm on $(xx, y_{\pi}^{r})$ with weights $w_{\pi}^{r}$ and estimate $\hat{\pi}_{1}^{r}(x)$ by the fitted probability of the first $n$ units.

Similarly, to fit $\mu(x)$ in the $r$-th iteration, one can construct a pseudo-dataset with outcomes $$y_{\mu}^{r} = (p_{1},\ldots, p_{n}, 1-p_{1}, \ldots, 1-p_{n}),$$
with weights 
$$w_{\mu}^{r} = \left(\hat{b}_{1}^{r}\cdot \hat{H}_{1}^{r}, \ldots, \hat{b}_{n}^{r}\cdot \hat{H}_{n}^{r}, (1-\hat{b}_{1}^{r})\cdot \hat{H}_{1}^{r}, \ldots, (1-\hat{b}_{n}^{r})\cdot \hat{H}_{n}^{r}\right).$$ 
Then one can apply any algorithm on $(xx, y_{\mu}^{r})$ with weights $w_{\mu}^{r}$ and estimate $\hat{\mu}(x)$ by the fitted values of the first $n$ units. Note that this is a more general version that Algorithm 2 in our paper. It is straightforward to show that for generalized linear models, this is equivalent to Algorithm 2. Also, as mentioned in Appendix A.3 in the supplementary material, we found that it yields a more stable performance by replacing the weights to
$$w_{\mu}^{r} = \left(\hat{b}_{1}^{r}, \ldots, \hat{b}_{n}^{r}, 1-\hat{b}_{1}^{r}, \ldots, 1-\hat{b}_{n}^{r}\right).$$

Note that $\hat{H}_{i}^{r}$ and $\hat{b}_{i}^{r}$ are generated within the algorithm so the user do not need to take care of them. Rather, the user only need to specifies the functions `pifun` and `mufun` as if $\hat{H}_{i}^{r}$ and $\hat{b}_{i}^{r}$ are known. More specifically, both `pifun` and `mufun` take $x$ and some output that has been calculated in the algorithm ($y_{\pi}^{r}$ or $y_{\mu}^{r}$) with some weights that also has been calculated in the algorithm ($w_{\pi}^{r}$ or $w_{\mu}^{r}$). They both output a sequence of estimates ($\pi_{1}(x)$ or $\mu(x)$) of length $n$, as well as some other information (e.g. degree of freedom, for performing model selection). 

In `adaptMT` package, two types of inputs are accepted for `pifun` and `mufun`: `list(formula = , data = , weights = , ...)` or `list(x = , y = , weights = , ...)`. We refer to them as formula-data input type and x-y input type. Both of them should have ouput in the form of `list(fitv = , info = )` where `fitv` gives the fitted values and `info` is a list that gives other information, including degree-of-freedom (`info$df`), variable importance (`info$vi`), etc.. If model selection is performed, the entry `info$df` must be given.

#### formula-data input type
For the former, the user only needs to provide a partial formula without a response name. For instance, if the covariate `x` (in the input of `adapt`) is a data.frame that has a single column named "x" and one wants to fit $\pi_{1}(x)$ using a logistic-Gamma GLM with 8 knots, the formula should be specified as `"ns(x, df = 8)"` or `"~ ns(x, df = 8)"` or `~ ns (x, df = 8)`. Then `adapt` automatically construct a data.frame in the form of data.frame(x = xx, \*\* = ...), where \*\* is a response name that is automatically generated by the algorithm which is guaranteed not to be conflicted with any variable names in `x`, and ... is the outcome $y_{\pi}^{r}$ or $y_{\mu}^{r}$. For technical reasons (unpleasant feature of `formula` objects in `R`), the input `formula` in `pifun` or `mufun` is always **assumed to be a character**. However, this technical issue can be simply avoid by adding a line at the beginning of `pifun` or `mufun`. See the example below.

For instance, in logistic-Gamma GLM, `pifun` is a logistic regression (of $y_{\pi}^{r}$ on $xx$ with weights $w_{\pi}^{r}$) and `mufun` is a Gamma GLM (or $y_{\mu}^{r}$ on $xx$ with weights $w_{\mu}^{r}$). Note that there is subtlety in fitting GLM using `glm` function in R `stats` package with `inverse` or `log` link functions (see `?glm` and `?family` for details), for which the initialization might fail. As an alternative choice, we define `safe_glm` as a function that first runs `glm` with the automatic initialization and, if it fails, runs `glm` with the manual initialization with intercept being 1 and all other entries being 0. The code is given as follows:

```{r}
safe_glm <- function(formula, family, data, weights = NULL, ...){
    options(warn = -1) # Turn off the warning

    formula <- as.formula(formula) 
    # transform the input 'formula' from a string to a 'formula' object
    
    if (family$link %in% c("inverse", "log")){
        fit <- try(glm(formula, family, data, weights, ...),
                   silent = TRUE)
        if (class(fit)[1] == "try-error"){
            mod_mat <- model.matrix(formula, data = data) 
            # get the covariate matrix, which might be different from x when, 
            # for instance, the formula involves ns() or s().
            p <- ncol(mod_mat) - 1 #
            start <- c(1, rep(0, p))
            fit <- glm(formula, family, data, weights,
                       start = start, ...)
        }
    } else {
        fit <- glm(formula, family, data, weights, ...)
    }

    fitv <- as.numeric(predict(fit, type = "response")) # return 'fitv'

    df <- fit$rank
    info <- list(df = df) # return 'info'. Here 'df' is given by the rank of covariate matrix

    options(warn = 0)
    
    return(list(fitv = fitv, info = info))
}
```

Then `pifun` and `mufun` can be defined simply as follows:
```{r}
pifun <- function(formula, data, weights, ...){
  safe_glm(formula, data, weights = weights,
           family = quasibinomial(), ...) 
  # Here quasibinomial() family gets rid of meaningless warning messages
}

mufun <- function(formula, data, weights, ...){
  safe_glm(formula, data, weights = weights,
           family = Gamma(), ...)
}
```
 

#### x-y input type

### Design principles and requirements for `pifun_init` and `mufun_init`
Finally we need to define `pifun_init` and `mufun_init` for the initialization. As described in the paper, the initialization of $\pi_{1}(x)$ is done by defining
$$J_{i} = 1 - \frac{I(s_{0}(x_{i}) \le p_{i}\le 1 - s_{0}(x_{i}))}{1 - 2s_{0}(x_i)}$$
and regress $J_{i}$ on $x_{i}$ with the same formula as in `pifun`. For `mufun_init` we simply impute the masked pvalues ad-hocly by $\min\{p_i, 1 - p_i\}$ and run `mufun` on the imputed data


## Other arguments

# Convenient End-to-End Wrappers
## Using `adapt_glm`



